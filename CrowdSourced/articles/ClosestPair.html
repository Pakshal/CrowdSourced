
<STYLE type="text/css">



#page_1 {position:relative; overflow: hidden;margin: 95px 0px 96px 96px;padding: 0px;border: none;width: 720px;}





#page_2 {position:relative; overflow: hidden;margin: 96px 0px 274px 96px;padding: 0px;border: none;width: 720px;}





#page_3 {position:relative; overflow: hidden;margin: 98px 0px 141px 96px;padding: 0px;border: none;width: 720px;}

#page_3 #dimg1 {z-index:2; }





#page_4 {position:relative; overflow: hidden;margin: 96px 0px 96px 96px;padding: 0px;border: none;width: 720px;}
#page_4 #id_1 {border:none;margin: 0px 0px 0px 0px;padding: 0px;border:none;width: 720px;overflow: hidden;}
#page_4 #id_2 {border:none;margin: 22px 0px 0px 0px;padding: 0px;border:none;width: 400px;overflow: hidden;}





#page_5 {position:relative; overflow: hidden;margin: 94px 0px 127px 96px;padding: 0px;border: none;width: 720px;}





#page_6 {position:relative; overflow: hidden;margin: 96px 0px 124px 96px;padding: 0px;border: none;width: 720px;}





#page_7 {position:relative; overflow: hidden;margin: 96px 0px 137px 96px;padding: 0px;border: none;width: 720px;}





#page_8 {position:relative; overflow: hidden;margin: 103px 0px 101px 96px;padding: 0px;border: none;width: 720px;}





#page_9 {position:relative; overflow: hidden;margin: 1056px 0px 0px 0px;padding: 0px;border: none;width: 0px;height: 0px;}





.dclr {clear:both;float:none;height:1px;margin:0px;padding:0px;overflow:hidden;}

.ft0{font: bold 24px 'Arial';line-height: 29px;}
.ft1{font: 15px 'Arial';line-height: 21px;}
.ft2{font: 14px 'Arial';line-height: 20px;}
.ft3{font: 15px 'Arial';line-height: 20px;}
.ft4{font: bold 19px 'Arial';line-height: 22px;}
.ft5{font: italic bold 15px 'Arial';line-height: 21px;}
.ft6{font: italic bold 15px 'Arial';line-height: 22px;}
.ft7{font: 9px 'Arial';line-height: 17px;position: relative; bottom: -3px;}
.ft8{font: italic bold 16px 'Arial';line-height: 23px;}
.ft9{font: 10px 'Arial';line-height: 18px;position: relative; bottom: -3px;}
.ft10{font: italic bold 10px 'Arial';line-height: 16px;position: relative; bottom: -3px;}
.ft11{font: italic bold 9px 'Arial';line-height: 16px;position: relative; bottom: -3px;}
.ft12{font: 15px 'MS PGothic';line-height: 20px;}
.ft13{font: 15px 'Arial';line-height: 22px;}
.ft14{font: italic bold 13px 'Arial';line-height: 21px;}
.ft15{font: 13px 'Arial';line-height: 21px;}
.ft16{font: italic bold 9px 'Arial';line-height: 15px;position: relative; bottom: -3px;}
.ft17{font: 10px 'Arial';line-height: 17px;position: relative; bottom: -3px;}
.ft18{font: 9px 'Arial';line-height: 16px;position: relative; bottom: -3px;}
.ft19{font: 9px 'Arial';line-height: 17px;position: relative; bottom: 6px;}
.ft20{font: 9px 'Arial';line-height: 12px;position: relative; bottom: -3px;}
.ft21{font: 15px 'Arial';line-height: 17px;}
.ft22{font: italic bold 15px 'Arial';line-height: 17px;}
.ft23{font: bold 9px 'Arial';line-height: 11px;position: relative; bottom: -3px;}
.ft24{font: bold 15px 'Arial';line-height: 18px;}
.ft25{font: italic bold 9px 'Arial';line-height: 11px;position: relative; bottom: -3px;}
.ft26{font: 15px 'Arial';line-height: 17px;}
.ft27{font: 29px 'Arial';line-height: 33px;position: relative; bottom: -6px;}
.ft28{font: 11px 'Arial';line-height: 14px;position: relative; bottom: 6px;}
.ft29{font: 9px 'Arial';line-height: 18px;position: relative; bottom: -3px;}
.ft30{font: italic bold 15px 'Arial';line-height: 23px;}
.ft31{font: 14px 'Arial';line-height: 22px;}
.ft32{font: 15px 'Arial';line-height: 23px;}
.ft33{font: bold 15px 'Arial';line-height: 21px;}
.ft34{font: italic 15px 'MS PGothic';line-height: 18px;}
.ft35{font: 15px 'Arial';line-height: 20px;}
.ft36{font: italic bold 15px 'Arial';line-height: 20px;}
.ft37{font: 15px 'MS PGothic';line-height: 15px;}
.ft38{font: 9px 'Arial';line-height: 12px;position: relative; bottom: -3px;}
.ft39{font: 9px 'Arial';line-height: 16px;position: relative; bottom: -3px;}
.ft40{font: italic bold 15px 'Arial';line-height: 21px;}
.ft41{font: 15px 'Arial';line-height: 21px;}
.ft42{font: 9px 'Arial';line-height: 17px;position: relative; bottom: -3px;}
.ft43{font: 15px 'Arial';line-height: 22px;}
.ft44{font: 15px 'MS PGothic';line-height: 19px;}
.ft45{font: italic bold 15px 'Arial';line-height: 20px;}
.ft46{font: 9px 'Arial';line-height: 15px;position: relative; bottom: -3px;}
.ft47{font: 15px 'MS PGothic';line-height: 18px;}
.ft48{font: italic bold 14px 'Arial';line-height: 20px;}
.ft49{font: 8px 'Arial';line-height: 15px;position: relative; bottom: -3px;}
.ft50{font: 14px 'MS PGothic';line-height: 19px;}
.ft51{font: 9px 'Arial';line-height: 16px;position: relative; bottom: 6px;}
.ft52{font: italic bold 13px 'Arial';line-height: 20px;}
.ft53{font: 7px 'Arial';line-height: 12px;position: relative; bottom: -3px;}
.ft54{font: 13px 'Arial';line-height: 20px;}
.ft55{font: bold 15px 'Arial';line-height: 18px;}
.ft56{font: italic bold 13px 'Courier New';line-height: 16px;}
.ft57{font: italic bold 8px 'Courier New';line-height: 11px;position: relative; bottom: -2px;}
.ft58{font: 13px 'Courier New';line-height: 16px;}
.ft59{font: italic bold 13px 'Courier New';line-height: 13px;}
.ft60{font: 13px 'Courier New';line-height: 13px;}
.ft61{font: italic bold 8px 'Courier New';line-height: 11px;}
.ft62{font: 1px 'Arial';line-height: 1px;}
.ft63{font: 13px 'Courier New';line-height: 11px;}
.ft64{font: 8px 'Courier New';line-height: 7px;}
.ft65{font: 1px 'Arial';line-height: 7px;}
.ft66{font: italic bold 12px 'Courier New';line-height: 16px;}
.ft67{font: 12px 'Courier New';line-height: 15px;}
.ft68{font: italic bold 13px 'Courier New';line-height: 7px;}
.ft69{font: 13px 'Courier New';line-height: 7px;}
.ft70{font: 13px 'Courier New';line-height: 18px;}
.ft71{font: italic bold 13px 'Courier New';line-height: 11px;}
.ft72{font: 13px 'Courier New';line-height: 22px;}
.ft73{font: bold 13px 'Arial';line-height: 22px;}
.ft74{font: italic bold 13px 'Arial';line-height: 22px;}
.ft75{font: italic bold 14px 'Arial';line-height: 21px;}
.ft76{font: 14px 'Arial';line-height: 21px;}
.ft77{font: 9px 'Arial';line-height: 15px;position: relative; bottom: 6px;}
.ft78{font: italic bold 9px 'Arial';line-height: 11px;position: relative; bottom: -3px;}
.ft79{font: italic bold 15px 'Arial';line-height: 17px;}
.ft80{font: bold 15px 'Arial';line-height: 22px;}
.ft81{font: italic bold 11px 'Arial';line-height: 13px;}
.ft82{font: bold 15px 'Arial';line-height: 21px;}
.ft83{font: 13px 'Arial';line-height: 22px;}
.ft84{font: italic bold 7px 'Arial';line-height: 12px;position: relative; bottom: -3px;}
.ft85{font: 8px 'Arial';line-height: 15px;position: relative; bottom: 6px;}
.ft86{font: 11px 'Arial';line-height: 13px;}
.ft87{font: 11px 'Arial';line-height: 14px;}
.ft88{font: 8px 'Arial';line-height: 10px;position: relative; bottom: -3px;}
.ft89{font: 14px 'Arial';line-height: 16px;}
.ft90{font: 36px 'Arial';line-height: 41px;position: relative; bottom: -6px;}
.ft91{font: italic bold 9px 'Arial';line-height: 11px;position: relative; bottom: -13px;}
.ft92{font: 13px 'Arial';line-height: 16px;}
.ft93{font: 38px 'Arial';line-height: 43px;}
.ft94{font: 9px 'Arial';line-height: 12px;position: relative; bottom: -2px;}
.ft95{font: 37px 'Arial';line-height: 42px;}
.ft96{font: italic bold 10px 'Arial';line-height: 11px;position: relative; bottom: -6px;}
.ft97{font: 10px 'Arial';line-height: 13px;position: relative; bottom: -6px;}
.ft98{font: 14px 'Arial';line-height: 16px;position: relative; bottom: 7px;}
.ft99{font: 14px 'Arial';line-height: 16px;position: relative; bottom: 6px;}
.ft100{font: italic bold 14px 'Arial';line-height: 16px;position: relative; bottom: 6px;}
.ft101{font: 11px 'Arial';line-height: 14px;position: relative; bottom: -12px;}

.p0{text-align: left;margin-top: 0px;margin-bottom: 0px;}
.p1{text-align: justify;padding-right: 67px;margin-top: 38px;margin-bottom: 0px;}
.p2{text-align: justify;padding-right: 63px;margin-top: 14px;margin-bottom: 0px;}
.p3{text-align: left;padding-right: 68px;margin-top: 23px;margin-bottom: 0px;}
.p4{text-align: left;margin-top: 19px;margin-bottom: 0px;}
.p5{text-align: left;padding-right: 74px;margin-top: 5px;margin-bottom: 0px;}
.p6{text-align: left;padding-right: 72px;margin-top: 18px;margin-bottom: 0px;}
.p7{text-align: justify;padding-right: 62px;margin-top: 13px;margin-bottom: 0px;}
.p8{text-align: left;margin-top: 15px;margin-bottom: 0px;}
.p9{text-align: justify;padding-right: 64px;margin-top: 5px;margin-bottom: 0px;}
.p10{text-align: left;padding-right: 77px;margin-top: 0px;margin-bottom: 0px;}
.p11{text-align: left;padding-right: 62px;margin-top: 18px;margin-bottom: 0px;}
.p12{text-align: left;padding-right: 67px;margin-top: 18px;margin-bottom: 0px;}
.p13{text-align: justify;padding-right: 70px;margin-top: 15px;margin-bottom: 0px;}
.p14{text-align: justify;padding-right: 72px;margin-top: 15px;margin-bottom: 0px;}
.p15{text-align: justify;padding-right: 73px;margin-top: 17px;margin-bottom: 0px;}
.p16{text-align: left;margin-top: 14px;margin-bottom: 0px;}
.p17{text-align: justify;padding-right: 74px;margin-top: 23px;margin-bottom: 0px;}
.p18{text-align: left;padding-left: 5px;margin-top: 377px;margin-bottom: 0px;}
.p19{text-align: left;margin-top: 42px;margin-bottom: 0px;}
.p20{text-align: left;margin-top: 22px;margin-bottom: 0px;}
.p21{text-align: left;margin-top: 2px;margin-bottom: 0px;}
.p22{text-align: left;padding-right: 91px;margin-top: 35px;margin-bottom: 0px;}
.p23{text-align: left;padding-right: 115px;margin-top: 16px;margin-bottom: 0px;}
.p24{text-align: left;margin-top: 18px;margin-bottom: 0px;}
.p25{text-align: justify;padding-right: 81px;margin-top: 3px;margin-bottom: 0px;}
.p26{text-align: justify;padding-right: 81px;margin-top: 18px;margin-bottom: 0px;}
.p27{text-align: justify;padding-right: 72px;margin-top: 0px;margin-bottom: 0px;}
.p28{text-align: justify;padding-right: 69px;margin-top: 16px;margin-bottom: 0px;}
.p29{text-align: justify;padding-right: 68px;margin-top: 15px;margin-bottom: 0px;}
.p30{text-align: justify;padding-right: 73px;padding-top: 1px;margin-top: 5px;margin-bottom: 0px;}
.p31{text-align: justify;padding-right: 72px;margin-top: 14px;margin-bottom: 0px;}
.p32{text-align: left;padding-right: 80px;margin-top: 1px;margin-bottom: 0px;}
.p33{text-align: justify;padding-right: 72px;margin-top: 18px;margin-bottom: 0px;}
.p34{text-align: left;margin-top: 20px;margin-bottom: 0px;}
.p35{text-align: left;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p36{text-align: left;padding-left: 140px;margin-top: 0px;margin-bottom: 0px;}
.p37{text-align: left;padding-left: 48px;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p38{text-align: left;padding-left: 8px;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p39{text-align: left;padding-left: 243px;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p40{text-align: left;padding-left: 96px;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p41{text-align: left;padding-left: 96px;margin-top: 0px;margin-bottom: 0px;}
.p42{text-align: left;padding-left: 283px;margin-top: 0px;margin-bottom: 0px;}
.p43{text-align: left;padding-left: 144px;padding-right: 103px;margin-top: 0px;margin-bottom: 0px;}
.p44{text-align: left;padding-left: 48px;margin-top: 0px;margin-bottom: 0px;}
.p45{text-align: left;padding-left: 158px;margin-top: 0px;margin-bottom: 0px;}
.p46{text-align: left;padding-right: 628px;margin-top: 0px;margin-bottom: 0px;text-indent: 48px;}
.p47{text-align: left;margin-top: 9px;margin-bottom: 0px;}
.p48{text-align: justify;padding-right: 74px;margin-top: 25px;margin-bottom: 0px;}
.p49{text-align: justify;padding-right: 72px;margin-top: 16px;margin-bottom: 0px;}
.p50{text-align: left;padding-right: 104px;margin-top: 17px;margin-bottom: 0px;}
.p51{text-align: justify;padding-right: 70px;margin-top: 16px;margin-bottom: 0px;}
.p52{text-align: justify;padding-right: 61px;margin-top: 20px;margin-bottom: 0px;}
.p53{text-align: left;padding-right: 77px;padding-top: 1px;margin-top: 0px;margin-bottom: 0px;}
.p54{text-align: justify;padding-right: 71px;margin-top: 19px;margin-bottom: 0px;}
.p55{text-align: left;margin-top: 17px;margin-bottom: 0px;}
.p56{text-align: justify;padding-right: 71px;padding-top: 1px;margin-top: 22px;margin-bottom: 0px;}
.p57{text-align: justify;padding-right: 75px;margin-top: 22px;margin-bottom: 0px;}
.p58{text-align: left;padding-right: 70px;margin-top: 15px;margin-bottom: 0px;}
.p59{text-align: left;margin-top: 28px;margin-bottom: 0px;}
.p60{text-align: left;padding-left: 74px;margin-top: 3px;margin-bottom: 0px;}
.p61{text-align: left;padding-right: 111px;margin-top: 23px;margin-bottom: 0px;}
.p62{text-align: justify;padding-right: 62px;margin-top: 30px;margin-bottom: 0px;}
.p63{text-align: left;margin-top: 1px;margin-bottom: 0px;}
.p64{text-align: left;padding-right: 64px;margin-top: 23px;margin-bottom: 0px;}
.p65{text-align: left;padding-right: 76px;margin-top: 0px;margin-bottom: 0px;}
.p66{text-align: justify;padding-right: 61px;margin-top: 16px;margin-bottom: 0px;}
.p67{text-align: justify;padding-right: 111px;margin-top: 22px;margin-bottom: 0px;}
.p68{text-align: justify;padding-right: 74px;margin-top: 16px;margin-bottom: 0px;}
.p69{text-align: justify;padding-right: 61px;margin-top: 11px;margin-bottom: 0px;}
.p70{text-align: justify;padding-right: 74px;margin-top: 2px;margin-bottom: 0px;}
.p71{text-align: left;padding-left: 271px;margin-top: 0px;margin-bottom: 0px;}
.p72{text-align: left;padding-left: 273px;margin-top: 0px;margin-bottom: 0px;}
.p73{text-align: left;margin-top: 23px;margin-bottom: 0px;}
.p74{text-align: left;padding-left: 1px;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p75{text-align: left;padding-left: 29px;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p76{text-align: left;padding-left: 6px;margin-top: 0px;margin-bottom: 0px;white-space: nowrap;}
.p77{text-align: left;padding-left: 112px;margin-top: 3px;margin-bottom: 0px;}
.p78{text-align: left;padding-left: 96px;margin-top: 30px;margin-bottom: 0px;}
.p79{text-align: left;padding-right: 79px;margin-top: 76px;margin-bottom: 0px;}
.p80{text-align: justify;padding-right: 71px;margin-top: 18px;margin-bottom: 0px;}
.p81{text-align: left;padding-right: 116px;margin-top: 20px;margin-bottom: 0px;}
.p82{text-align: justify;padding-right: 68px;margin-top: 19px;margin-bottom: 0px;}
.p83{text-align: justify;padding-right: 68px;margin-top: 16px;margin-bottom: 0px;}
.p84{text-align: left;padding-right: 55px;margin-top: 1px;margin-bottom: 0px;}
.p85{text-align: justify;padding-right: 60px;margin-top: 18px;margin-bottom: 0px;}

.td0{padding: 0px;margin: 0px;width: 384px;vertical-align: bottom;}
.td1{padding: 0px;margin: 0px;width: 16px;vertical-align: bottom;}
.td2{padding: 0px;margin: 0px;width: 0px;vertical-align: bottom;}
.td3{padding: 0px;margin: 0px;width: 260px;vertical-align: bottom;}
.td4{padding: 0px;margin: 0px;width: 86px;vertical-align: bottom;}
.td5{padding: 0px;margin: 0px;width: 254px;vertical-align: bottom;}
.td6{padding: 0px;margin: 0px;width: 346px;vertical-align: bottom;}
.td7{padding: 0px;margin: 0px;width: 600px;vertical-align: bottom;}
.td8{padding: 0px;margin: 0px;width: 410px;vertical-align: bottom;}
.td9{padding: 0px;margin: 0px;width: 231px;vertical-align: bottom;}
.td10{padding: 0px;margin: 0px;width: 184px;vertical-align: bottom;}
.td11{padding: 0px;margin: 0px;width: 226px;vertical-align: bottom;}

.tr0{height: 24px;}
.tr1{height: 20px;}
.tr2{height: 26px;}
.tr3{height: 6px;}
.tr4{height: 11px;}
.tr5{height: 5px;}
.tr6{height: 12px;}
.tr7{height: 7px;}
.tr8{height: 18px;}
.tr9{height: 53px;}
.tr10{height: 17px;}
.tr11{height: 66px;}
.tr12{height: 49px;}

.t0{width: 400px;font: italic bold 8px 'Courier New';}
.t1{width: 600px;font: 13px 'Courier New';}
.t2{width: 641px;font: 15px 'Arial';}

</STYLE>


<DIV id="page_1">


<P class="p0 ft0">Finding the Closest Pair of Points: A Randomized Approach</P>
<P class="p1 ft1">In this article, we will show how to use randomization to develop an algorithm for the closest pair problem, using an underlying dictionary data structure. We will show that this algorithm runs in O(n) expected time, plus O(n) expected dictionary operations.</P>
<P class="p2 ft2">There are several related reasons why it is useful to express the running time of our algorithm in this way, accounting for the dictionary operations separately. We know that dictionaries have a very efficient implementation using hashing, so abstracting out the dictionary operations allows us to treat the hashing as a "black box" and have the algorithm inherit an overall running time from whatever performance guarantee is satisfied by this hashing procedure. A concrete payoff of this is the following. It has been shown that with the right choice of hashing procedure, one can make the underlying dictionary operations run in linear expected time as well, yielding an overall expected running time of O(n). We will talk about the ideas that lead to this O(n) bound at the end of this section.</P>
<P class="p3 ft3">It is worth remarking at the outset that randomization shows up for two independent reasons in this algorithm: the way in which the algorithm processes the input points will have a random component, regardless of how the dictionary data structure is implemented; and when the dictionary is implemented using hashing, this introduces an additional source of randomness as part of the hash­table operations. Expressing the running time via the number of dictionary operations allows us to cleanly separate the two uses of randomness.</P>
<P class="p4 ft4">The Problem</P>
<P class="p5 ft1">We are given n points in the plane, and we wish to find the pair that is closest together. This is one of the most basic geometric <SPAN class="ft5">proximity </SPAN>problems, a topic with a wide range of applications.</P>
<P class="p6 ft13">We will use the same notation as in our earlier discussion of the closest­pair problem. We will denote the set of points by P = { <SPAN class="ft6">p</SPAN><SPAN class="ft7">1 </SPAN>..... <SPAN class="ft6">p</SPAN><SPAN class="ft7">n</SPAN>}, where <SPAN class="ft8">p</SPAN><SPAN class="ft9">i </SPAN>has coordinates (<SPAN class="ft8">x</SPAN><SPAN class="ft10">i </SPAN><SPAN class="ft8">, y</SPAN><SPAN class="ft10">i</SPAN>); and for two points <SPAN class="ft6">p</SPAN><SPAN class="ft11">i </SPAN><SPAN class="ft6">, p</SPAN><SPAN class="ft11">j </SPAN><SPAN class="ft12">∈ </SPAN>P, we use <SPAN class="ft6">d</SPAN>(<SPAN class="ft6">p</SPAN><SPAN class="ft11">i</SPAN><SPAN class="ft6">, p</SPAN><SPAN class="ft11">j</SPAN>) to denote the standard Euclidean distance between them. Our goal is to find the pair of points <SPAN class="ft6">p</SPAN><SPAN class="ft11">i </SPAN><SPAN class="ft6">, p</SPAN><SPAN class="ft11">j </SPAN>that minimizes <SPAN class="ft6">d</SPAN>(<SPAN class="ft6">p</SPAN><SPAN class="ft11">i </SPAN><SPAN class="ft6">, p</SPAN><SPAN class="ft11">j</SPAN>).</P>
<P class="p7 ft1">To simplify the discussion, we will assume that the points are all in the unit square: 0 ≤ xi, yi &lt; 1 for all i = 1, ..... ,n. This is no loss of generality: in linear time, we can rescale all the x­ and y­coordinates of the points so that they lie in a unit square, and then we can translate them so that this unit square has its lower left corner: at the origin.</P>
<P class="p8 ft4">Designing the Algorithm</P>
<P class="p9 ft15">The basic idea of the algorithm is very simple. We’ll consider the points in random order, and maintain a current value δ for the closest pair as we process the points in this order. When we get to a new point <SPAN class="ft14">p</SPAN>, we look "in the vicinity" of <SPAN class="ft14">p </SPAN>to see if any of the previously considered points are at a distance less than δ from p. If not, then the closest pair hasn’t changed, and we move on to the next point in the random order.</P>
</DIV>
<DIV id="page_2">


<P class="p10 ft1">If there is a point within a distance less than δ from <SPAN class="ft5">p</SPAN>, then the closest pair has changed, and we will need to update it.</P>
<P class="p11 ft1">The challenge in turning this into an efficient algorithm is to figure out how to implement the task of looking for points in the vicinity of <SPAN class="ft5">p</SPAN>. It is here that the dictionary, data structure will come into play.</P>
<P class="p12 ft1">We now begin making this more concrete. Let us assume for simplicity that the points in our random order are labeled <SPAN class="ft5">p</SPAN><SPAN class="ft16">1</SPAN><SPAN class="ft5">, . . . , p</SPAN><SPAN class="ft16">n</SPAN>. The algorithm proceeds in stages; during each stage, the closest pair remains constant. The first stage starts by setting δ = <SPAN class="ft5">d</SPAN>(<SPAN class="ft5">p</SPAN><SPAN class="ft16">l </SPAN><SPAN class="ft5">, p</SPAN><SPAN class="ft16">2</SPAN>), the distance of the first two points. The goal of a stage is to either verify that δ is indeed the distance of the closest pair of points, or to find a pair of points <SPAN class="ft5">p</SPAN><SPAN class="ft16">i</SPAN>, <SPAN class="ft5">p</SPAN><SPAN class="ft16">j </SPAN>with <SPAN class="ft5">d</SPAN>(<SPAN class="ft5">p</SPAN><SPAN class="ft16">i</SPAN>, <SPAN class="ft5">p</SPAN><SPAN class="ft16">j</SPAN>) &lt; δ. During a stage, we’ll gradually add points in the order <SPAN class="ft5">p</SPAN><SPAN class="ft16">1</SPAN>, <SPAN class="ft5">p</SPAN><SPAN class="ft16">2</SPAN>, . . . ,<SPAN class="ft5">p</SPAN><SPAN class="ft16">n</SPAN>. The stage terminates when we reach a point <SPAN class="ft5">p</SPAN><SPAN class="ft16">i </SPAN>so that for some <SPAN class="ft5">j </SPAN>&lt; <SPAN class="ft5">i</SPAN>, we have <SPAN class="ft5">d</SPAN>(<SPAN class="ft5">p</SPAN><SPAN class="ft16">i</SPAN>, <SPAN class="ft5">p</SPAN><SPAN class="ft16">j</SPAN>) &lt; δ. We then let δ for the next stage be the closest distance found so far: δ = min<SPAN class="ft17">j:j&lt;i </SPAN><SPAN class="ft5">d</SPAN>(<SPAN class="ft5">p</SPAN><SPAN class="ft16">i</SPAN>, <SPAN class="ft5">p</SPAN><SPAN class="ft16">j</SPAN>).</P>
<P class="p13 ft1">The number of stages used will depend on the random order. If we get lucky, and p<SPAN class="ft18">l</SPAN>, p<SPAN class="ft18">2 </SPAN>are the closest pair of points, then a single stage will do. It is also possible to have as many as n ­ 2 stages, if adding a new point always decreases the minimum distance. We’ll show that the expected running time of the algorithm is within a constant factor of the time needed in the first, lucky case, when the original value of δ is the smallest distance.</P>
<P class="p14 ft1"><SPAN class="ft5">Testing a Proposed Distance: </SPAN>The main subroutine of the algorithm is a method to test whether the current pair of points with distance δ remains the closest pair when a new point is added and, if not, to find the new closest pair.</P>
<P class="p15 ft13">The idea of the verification is to subdivide the unit square (the area where the points lie) into subsquares, whose sides have length δ/2, as shown in Figure 13.2. Formally, there will be N<SPAN class="ft19">2 </SPAN>subsquares,where N= 1/(2δ): for 0 ≤ s≤ N ­ 1 and 1 ≤ t ≤ N ­ 1, we define the subsquare S<SPAN class="ft7">st </SPAN>as</P>
<P class="p16 ft21">S<SPAN class="ft20">st </SPAN>= {(x, y) : sδ/2 ≤ x &lt; (s + 1)δ/2; tδ/2≤ y &lt; (t + 1)δ/2}.</P>
<P class="p17 ft1">We claim that this collection of subsquares has two nice properties for our purposes. First, any two points that lie in the same subsquare have distance less than δ. Second, and a partial converse to this, any two points that are less than δ away from each other must fall in either the same subsquare or in very close subsquares.</P>
</DIV>
<DIV id="page_3">
<DIV id="dimg1">
<IMG src="./articles/closest_pair_images/closest_pair.jpg" id="img1"></IMG>

</DIV>


<DIV class="dclr"></DIV>
<P class="p18 ft24">Figure 13.2 Dividing the square into size δ/2 subsquares. The point <SPAN class="ft22">p </SPAN>lies in the subsquare S<SPAN class="ft23">st.</SPAN></P>
<P class="p19 ft22"><SPAN class="ft24">(13.26) </SPAN>If two points p and q belong to the same subsquare S<SPAN class="ft25">st </SPAN>, then d(p,q) &lt; <SPAN class="ft26">δ</SPAN>.</P>
<P class="p20 ft26"><SPAN class="ft24">Proof</SPAN>. If points <SPAN class="ft22">p </SPAN>and <SPAN class="ft22">q </SPAN>are in the same subsquare, then both coordinates of the two points differ by at</P>
<P class="p21 ft26">most δ/2, and hence <SPAN class="ft22">d</SPAN>(<SPAN class="ft22">p</SPAN>, <SPAN class="ft22">q</SPAN>) ≤ <SPAN class="ft27">√</SPAN>(δ/2)<SPAN class="ft28">2 </SPAN>+ (δ/2)<SPAN class="ft28">2 </SPAN>= δ/√2 &lt; δ,as required.</P>
<P class="p22 ft32">Next, we say that subsquares S<SPAN class="ft29">st </SPAN>and S<SPAN class="ft29">s’t’</SPAN>, are <SPAN class="ft30">close </SPAN>if |<SPAN class="ft30">s </SPAN>−<SPAN class="ft30">s</SPAN><SPAN class="ft31">′</SPAN>|≤ 2 and |<SPAN class="ft30">t </SPAN>−<SPAN class="ft30">t</SPAN><SPAN class="ft31">′</SPAN>|≤ 2. (Note that a subsquare is close to itself.)</P>
<P class="p23 ft36"><SPAN class="ft33">(13.27) </SPAN>If for two points p, q <SPAN class="ft34">∈ </SPAN>P we have d(p, q) &lt; <SPAN class="ft35">δ, </SPAN>then the subsquares containing them are close.</P>
<P class="p24 ft26"><SPAN class="ft24">Proof. </SPAN>Consider two points p, q <SPAN class="ft37">∈ </SPAN>P belonging to subsquares that are not close; assume p <SPAN class="ft37">∈ </SPAN>S<SPAN class="ft38">st </SPAN>and q <SPAN class="ft37">∈</SPAN></P>
<P class="p25 ft41">S<SPAN class="ft39">s’t’ </SPAN>,where one of s, s’ or t, t’ differs by more than 2. It follows that in one of their respective x­ or y­coordinates, p and q differ by at least δ, and so we cannot have d(<SPAN class="ft40">p</SPAN>, <SPAN class="ft40">q</SPAN>) &lt; δ.</P>
<P class="p26 ft43">Note that for any subsquare S<SPAN class="ft42">st</SPAN>, the set of subsquares dose to it form a 5 x 5 grid around it. Thus we conclude that there are at most 25 subsquares close to S<SPAN class="ft42">st</SPAN>, counting S<SPAN class="ft42">st </SPAN>itself. (There will be fewer than 25 if S<SPAN class="ft42">st </SPAN>is at the edge of the unit square containing the input points.)</P>
</DIV>
<DIV id="page_4">


<DIV id="id_1">
<P class="p27 ft1">Statements (13.26) and (13.27) suggest the basic outline of our algorithm. Suppose that, at some point in the algorithm, we have proceeded partway through the random order of the points and seen P’ <SPAN class="ft44">⊆ </SPAN>P, and suppose that we know the minimum distance among pomts in to be 3. For each of the points in P’, we keep track of the subsquare containing it.</P>
<P class="p28 ft3">Now, when the next point <SPAN class="ft45">p </SPAN>is considered, we determine which of the subsquares S<SPAN class="ft46">st </SPAN>it belongs to. If p is going to cause the minimum distance to change, there must be some earlier point p’ <SPAN class="ft47">∈ </SPAN>P’ at distance less than δ from it; and hence, by (13.27), the point <SPAN class="ft45">p’ </SPAN>must be in one of the 25 squares around the square S<SPAN class="ft46">st </SPAN>containing <SPAN class="ft45">p. </SPAN>So we will simply check each of these 25 squares one by one to see if it contains a point in P’; for each point in P’ that we find this way, we compute its distance to p. By (13.26), each of these subsquares contains at most one point of P’, so this is at most a constant number of distance computations. (Note that we used a similar idea, via (5.10), at a crucial point in the divide­and­conquer algorithm for this problem in Chapter 5.)</P>
<P class="p29 ft2"><SPAN class="ft48">A Data Structure for Maintaining the Subsquares: </SPAN>The high­level description of the algorithm relies on being able to name a subsquare S<SPAN class="ft49">st </SPAN>and quickly determine which points of P, if any, are contained in it. A dictionary is a natural data structure for implementing such operations. The universe U of possible elements is the set of all subsquares, and the set S maintained by the data structure will be the subsquares that contain points from among the set P’ that we’ve seen so far. Specifically, for each point <SPAN class="ft48">p’ </SPAN><SPAN class="ft50">∈ </SPAN>P’ that we have seen so far, we keep the subsquare containing it in the dictionary, tagged with the</P>
<P class="p30 ft1">index of <SPAN class="ft5">p’</SPAN>. We note that N<SPAN class="ft51">2 </SPAN>= [1/(2δ)]<SPAN class="ft51">2 </SPAN>will, in general, be much larger than n, the number of points. Thus we are in the type of situation considered in Section 13.6 on hashing, where the universe of possible elements (the set of all subsquares) is much larger than the number of elements being indexed (the subsquares containing an input point seen thus far).</P>
<P class="p31 ft54">Now, when we consider the next point <SPAN class="ft52">p </SPAN>in the random order, we determine the subsquare S<SPAN class="ft53">st </SPAN>containing it and perform a <SPAN class="ft52">Lookup </SPAN>operation for each of the 25 subsquares close to Sst. For any points discovered by these <SPAN class="ft52">Lookup </SPAN>operations, we compute the distance to p. If none of these distances are less</P>
<P class="p32 ft1">than δ, then the closest distance hasn’t changed; we insert S<SPAN class="ft18">st </SPAN>(tagged with <SPAN class="ft5">p</SPAN>) into the dictionary and proceed to the next point.</P>
<P class="p33 ft3">However, if we find a point <SPAN class="ft45">p’ </SPAN>such that δ<SPAN class="ft45">’ </SPAN>= <SPAN class="ft45">d</SPAN>(<SPAN class="ft45">p</SPAN>, <SPAN class="ft45">p’</SPAN>) &lt; δ, then we need to update our closest pair. This updating is a rather dramatic activity: Since the value of the closest pair has dropped from δ to δ’, our entire collection of subsquares, and the dictionary supporting it, has become useless­­it was, after all, designed only to be useful if the minimum distance was δ. We therefore invoke <SPAN class="ft45">MakeDictionary </SPAN>to create a new, empty dictionary that will hold subsquares whose side lengths are δ’/2. For each point seen thus far, we determine the subsquare containing it (in this new collection of subsquares), and we insert this subsquare into the dictionary. Having done all this, we are again ready to handle the next point in the random order.</P>
<P class="p34 ft21"><SPAN class="ft55">Summary of the Algorithm: </SPAN>We have now actually described the algorithm in full. To recap:</P>
</DIV>
<DIV id="id_2">
<TABLE cellpadding=0 cellspacing=0 class="t0">
<TR>
	<TD class="tr0 td0"><P class="p35 ft58">Order the points in a random sequence <SPAN class="ft56">p</SPAN><SPAN class="ft57">l</SPAN>,<SPAN class="ft56">p</SPAN><SPAN class="ft57">2 </SPAN>.....</P></TD>
	<TD class="tr0 td1"><P class="p35 ft56">p<SPAN class="ft57">n</SPAN></P></TD>
</TR>
</TABLE>
</DIV>
</DIV>
<DIV id="page_5">


<P class="p0 ft58">Let δ denote the minimum distance found so far</P>
<P class="p0 ft60">Initialize δ : <SPAN class="ft59">d</SPAN>(<SPAN class="ft59">p</SPAN>, <SPAN class="ft59">p</SPAN>)</P>
<P class="p36 ft61">l 2</P>
<P class="p0 ft58">Invoke <SPAN class="ft56">MakeDictionary </SPAN>for storing subsquares of side length δ/2</P>
<TABLE cellpadding=0 cellspacing=0 class="t1">
<TR>
	<TD class="tr1 td2"></TD>
	<TD class="tr1 td3"><P class="p35 ft56"><SPAN class="ft58">For </SPAN>i=1, 2 ..... n:</P></TD>
	<TD class="tr1 td4"><P class="p35 ft62">&nbsp;</P></TD>
	<TD rowspan=2 class="tr2 td5"><P class="p35 ft56">p</P></TD>
</TR>
<TR>
	<TD class="tr3 td2"></TD>
	<TD rowspan=2 class="tr4 td3"><P class="p37 ft63">Determine the subsquare S</P></TD>
	<TD rowspan=2 class="tr4 td4"><P class="p35 ft63">containing</P></TD>
</TR>
<TR>
	<TD class="tr5 td2"></TD>
	<TD rowspan=2 class="tr6 td5"><P class="p38 ft61">i</P></TD>
</TR>
<TR>
	<TD class="tr7 td2"></TD>
	<TD class="tr7 td3"><P class="p39 ft64">st</P></TD>
	<TD class="tr7 td4"><P class="p35 ft65">&nbsp;</P></TD>
</TR>
<TR>
	<TD class="tr8 td2"></TD>
	<TD colspan=2 class="tr8 td6"><P class="p37 ft58">Look up the 25 subsquares close to <SPAN class="ft56">p</SPAN><SPAN class="ft57">i</SPAN></P></TD>
	<TD class="tr8 td5"><P class="p35 ft62">&nbsp;</P></TD>
</TR>
<TR>
	<TD class="tr8 td2"></TD>
	<TD colspan=3 class="tr8 td7"><P class="p37 ft58">Compute the distance from <SPAN class="ft56">p</SPAN><SPAN class="ft57">i </SPAN>to any points found in these subsquares</P></TD>
</TR>
<TR>
	<TD class="tr8 td2"></TD>
	<TD colspan=3 class="tr8 td7"><P class="p37 ft58">If there is a point <SPAN class="ft56">p</SPAN><SPAN class="ft57">j </SPAN>(<SPAN class="ft56">j&lt;i</SPAN>) such that δ’=<SPAN class="ft56">d</SPAN>(<SPAN class="ft56">p</SPAN><SPAN class="ft57">j </SPAN><SPAN class="ft56">,p</SPAN><SPAN class="ft57">i</SPAN>) &lt; δ then</P></TD>
</TR>
<TR>
	<TD class="tr8 td2"></TD>
	<TD colspan=2 class="tr8 td6"><P class="p40 ft58">Delete the current dictionary</P></TD>
	<TD class="tr8 td5"><P class="p35 ft62">&nbsp;</P></TD>
</TR>
<TR>
	<TD class="tr8 td2"></TD>
	<TD colspan=3 class="tr8 td7"><P class="p40 ft67">Invoke <SPAN class="ft66">MakeDictionary </SPAN>for storing subsquares of side length δ’/2</P></TD>
</TR>
</TABLE>
<P class="p41 ft69">For each of the points <SPAN class="ft68">p,p,....p</SPAN>:</P>
<P class="p42 ft61">l 2 i</P>
<P class="p43 ft70">Determine the subsquare of side length δ’/2 that contains it Insert this subsquare into the new dictionary</P>
<P class="p41 ft58">End for</P>
<P class="p44 ft58">Else</P>
<P class="p41 ft63">Insert <SPAN class="ft71">p </SPAN>into the current dictionary</P>
<P class="p45 ft61">i</P>
<P class="p46 ft72">Endif Endfor</P>
<P class="p47 ft4">Analyzing the Algorithm</P>
<P class="p48 ft1">There are already some things we can say about the overall running time of the algorithm. To consider a new point Pi, we need to perform only a constant number of Lookup operations and a constant number of distance computations. Moreover, even if we had to update the closest pair in every iteration, we’d only do n <SPAN class="ft5">MakeDictionary </SPAN>operations.</P>
<P class="p49 ft1">The missing ingredient is the total expected cost, over the course of the algorithm’s execution, due to reinsertions into new dictionaries when the closest pair is updated. We will consider this next. For now, we can at least summarize the current state of our knowledge as follows:</P>
<P class="p50 ft74"><SPAN class="ft73">(13.28) </SPAN>The algorithm correctly maintains the closest pair at all times and it performs at most O(n) distance computations; O(n) Lookup operations, and O(n) MakeDictionary operations.</P>
<P class="p51 ft3">We now conclude the analysis by bounding the expected number of <SPAN class="ft45">Insert </SPAN>operations. Trying to find a good bound on the total expected number of <SPAN class="ft45">Insert </SPAN>operations seems a bit problematic at first: An update to the closest pair in iteration i will result in i insertions, and so each update comes at a high cost once i gets large. Despite this, we will show the surprising fact that the expected number of insertions is only O(n). The intuition here is that, even as the cost of updates becomes steeper as the iterations proceed, these updates become correspondingly less likely.</P>
<P class="p52 ft76">Let X be a random variable specifying the number of <SPAN class="ft75">Insert </SPAN>operations performed; the value of this random variable is determined by the random order chosen at the outset. We are interested in bounding E[X], and as usual in this type of situation, it is helpful to break X down into a sum of simpler random</P>
</DIV>
<DIV id="page_6">


<P class="p53 ft3">variables. Thus let X<SPAN class="ft46">i </SPAN>be a random variable equal to 1 if the i<SPAN class="ft77">th </SPAN>point in the random order causes the minimum distance to change, and equal to 0 otherwise.</P>
<P class="p54 ft1">Using these random variables X<SPAN class="ft18">i</SPAN>, we can write a simple formula for the total number of <SPAN class="ft5">Insert </SPAN>operations. Each point is inserted once when it is first encountered; and i points need to be reinserted if the minimum distance changes in iteration i. Thus we have the following claim.</P>
<P class="p55 ft79"><SPAN class="ft55">(13.29) </SPAN>The total number of Insert operations performed by the algorithm is n + Σ<SPAN class="ft78">i </SPAN>iX<SPAN class="ft78">i</SPAN>.</P>
<P class="p56 ft3">Now we bound the probability Pr [Xi = 1] that considering the i<SPAN class="ft77">th </SPAN>point causes the minimum distance to change.</P>
<P class="p4 ft21"><SPAN class="ft55">(13.30) </SPAN>Pr [X<SPAN class="ft20">i </SPAN>= 1] ≤ 2/i</P>
<P class="p57 ft1"><SPAN class="ft80">Proof. </SPAN>Consider the first i points <SPAN class="ft5">p</SPAN><SPAN class="ft16">1</SPAN><SPAN class="ft5">, p</SPAN><SPAN class="ft16">2 </SPAN><SPAN class="ft5">..... p</SPAN><SPAN class="ft16">i </SPAN>in the random order. Assume that the minimum distance among these points is achieved by p and q. Now the point <SPAN class="ft5">p</SPAN><SPAN class="ft16">i </SPAN>can only cause the minimum distance to decrease if <SPAN class="ft5">p</SPAN><SPAN class="ft16">i </SPAN>= <SPAN class="ft5">p </SPAN>or <SPAN class="ft5">p</SPAN><SPAN class="ft16">i </SPAN><SPAN class="ft5">= q. </SPAN>Since the first i points are in a random order, any of them is equally likely to be last, so the probability that p or q is last is 2/i.</P>
<P class="p58 ft1">Note that 2/i is only an upper bound in (13.30) because there could be multiple pairs among the first i points that define the same smallest distance.</P>
<P class="p24 ft21">By (13.29) and (13.30), we can bound the total number of <SPAN class="ft79">Insert </SPAN>operations as</P>
<P class="p59 ft21">E[X] = n + ∑i.E[x<SPAN class="ft20">i</SPAN>] ≤ n + 2n = 3n.</P>
<P class="p60 ft81">i</P>
<P class="p20 ft21">Combining this with (13.28), we obtain the following bound on the running time of the algorithm.</P>
<P class="p61 ft45"><SPAN class="ft82">(13.31) </SPAN>In expectation, the randomized closest­pair algorithm requires O(n) time plus O(n) dictionary operations.</P>
<P class="p24 ft4">Achieving Linear Expected Running Time</P>
<P class="p62 ft2">Up to this point, we have treated the dictionary data structure as a black box, and in (13.31) we bounded the running time of the algorithm in terms of computational time plus dictionary operations. We now want to give a bound on the actual expected running time, and so we need to analyze the work</P>
<P class="p63 ft21">involved in performing these dictionary operations.</P>
<P class="p64 ft83">To implement the dictionary, we’ll use a universal hashing scheme, like the one discussed in Section 13.6. Once the algorithm employs a hashing scheme, it is making use of randomness in two distinct ways: First,</P>
</DIV>
<DIV id="page_7">


<P class="p65 ft1">we randomly order the points to be added; and second, for each new minimum distance δ, we apply randomization to set up a new hash table using a universal hashing scheme.</P>
<P class="p66 ft54">When inserting a new point <SPAN class="ft52">p</SPAN><SPAN class="ft84">i</SPAN><SPAN class="ft52">, </SPAN>the algorithm uses the hash­table <SPAN class="ft52">Lookup </SPAN>operation to find all nodes in the 25 subsquares close to <SPAN class="ft52">p</SPAN><SPAN class="ft84">i</SPAN>. However, if the hash table has collisions, then these 25 Lookup operations can involve inspecting many more than 25 nodes. Statement (13.23) from Section ,13.6 shows that each such <SPAN class="ft52">Lookup </SPAN>operation involves considering O(1) previously inserted points, in expectation. It seems intuitively clear that performing O(n) hash­table operations in expectation, each of which involves considering O(1) elements in expectation, will result in an expected running time of O(n) overall. To make this intuition precise, we need to be careful with how these two sources of randomness interact.</P>
<P class="p67 ft5"><SPAN class="ft80">(13.32) </SPAN>Assume we implement the randomized closest­pair algorithm using a universal hashing scheme. In expectation, the total number of points considered during the Lookup operations is bounded by O(n).</P>
<P class="p68 ft1"><SPAN class="ft80">Proof. </SPAN>From (13.31) we know that the expected number of <SPAN class="ft5">Lookup </SPAN>operations is O(n), and from (13.23) we know that each of these <SPAN class="ft5">Lookup </SPAN>operations involves considering only O(1) points in expectation. In order to conclude that this implies the expected number of points considered is O(n), we now consider the relationship between these two sources of randomness.</P>
<P class="p69 ft2">Let X be a random variable denoting the number of <SPAN class="ft48">Lookup </SPAN>operations performed by the algorithm. Now the random order σ that the algorithm chooses for the points completely determines the sequence of minimum distance values the algorithm will consider and the sequence of dictionary operations it will perform. As a result, the choice of σ determines the value of X; we let X(σ) denote this value, and we let ξ<SPAN class="ft49">σ </SPAN>denote the event the algorithm chooses the random order or. Note that the conditional expectation E [X |ξ<SPAN class="ft49">σ </SPAN>] is equal to X(σ). Also, by (13.31), we know that E [X] ≤ c<SPAN class="ft49">0</SPAN>n, for some constant c<SPAN class="ft49">0</SPAN>.</P>
<P class="p70 ft2">Now consider this sequence of <SPAN class="ft48">Lookup </SPAN>operations for a fixed order σ . For i = 1 ..... X(σ), let Y<SPAN class="ft49">i </SPAN>be the number of points that need to be inspected during the i<SPAN class="ft85">th </SPAN><SPAN class="ft48">Lookup </SPAN>operations­­namely, the number of previously inserted points that collide with the dictionary entry involved in this <SPAN class="ft48">Lookup </SPAN>operation. We</P>
<P class="p71 ft86"><SPAN class="ft81">X</SPAN>(σ)</P>
<P class="p0 ft21">would like to bound the expected value of ∑ <SPAN class="ft79">Y i</SPAN>, where expectation is over both the random choice of</P>
<P class="p72 ft87"><SPAN class="ft81">i</SPAN>=1</P>
<P class="p63 ft21">σ and the random choice of hash function.</P>
<P class="p73 ft21">By (13.23), we know that E [Y<SPAN class="ft20">i</SPAN>|ξσ] = O(1) for all σ and all values of i. It is useful to be able to refer to</P>
<TABLE cellpadding=0 cellspacing=0 class="t2">
<TR>
	<TD colspan=2 class="tr0 td8"><P class="p35 ft89">the constant in the expression O(1) here, so we will say that E [Y<SPAN class="ft88">i</SPAN></P></TD>
	<TD class="tr0 td9"><P class="p35 ft21">| ξσ] ≤ c<SPAN class="ft20">1 </SPAN>for all σ and all values of i.</P></TD>
</TR>
<TR>
	<TD colspan=2 class="tr9 td8"><P class="p35 ft92">Summing over all i, and using linearity of expectation, we get E<SPAN class="ft90">[</SPAN>∑<SPAN class="ft91">i</SPAN></P></TD>
	<TD class="tr9 td9"><P class="p74 ft21"><SPAN class="ft79">Y i</SPAN>| ξσ<SPAN class="ft93">]</SPAN>≤ c<SPAN class="ft94">1</SPAN>.X(σ). Now we have</P></TD>
</TR>
<TR>
	<TD class="tr10 td10"><P class="p75 ft87"><SPAN class="ft81">X</SPAN>(σ )</P></TD>
	<TD rowspan=2 class="tr11 td11"><P class="p35 ft79">Y i<SPAN class="ft21">|ξσ</SPAN><SPAN class="ft93">]</SPAN></P></TD>
	<TD class="tr10 td9"><P class="p35 ft62">&nbsp;</P></TD>
</TR>
<TR>
	<TD class="tr12 td10"><P class="p76 ft100">E<SPAN class="ft95">[</SPAN><SPAN class="ft96">i</SPAN><SPAN class="ft97">=1</SPAN><SPAN class="ft98">∑ </SPAN>Y i<SPAN class="ft95">]</SPAN><SPAN class="ft98">=∑</SPAN><SPAN class="ft97">σ </SPAN>Pr<SPAN class="ft99">[ξσ]</SPAN>E<SPAN class="ft95">[</SPAN><SPAN class="ft98">∑</SPAN><SPAN class="ft96">i</SPAN></P></TD>
	<TD class="tr12 td9"><P class="p35 ft62">&nbsp;</P></TD>
</TR>
</TABLE>
</DIV>
<DIV id="page_8">


<P class="p41 ft21">≤ ∑<SPAN class="ft79">Pr</SPAN>[ ξσ].c<SPAN class="ft20">1</SPAN>X(σ)</P>
<P class="p77 ft87">σ</P>
<P class="p78 ft21">=c<SPAN class="ft20">1 </SPAN>∑<SPAN class="ft101">σ </SPAN><SPAN class="ft79">E</SPAN>[<SPAN class="ft79">X</SPAN>| ξσ].Pr[ξσ]=c<SPAN class="ft20">1</SPAN>E[X].</P>
<P class="p79 ft1">Since we know that E [X] is at most c<SPAN class="ft18">0</SPAN>n, the total expected number of points considered is at most c<SPAN class="ft18">0</SPAN>c<SPAN class="ft18">1</SPAN>n= O(n), which proves the claim.</P>
<P class="p80 ft3">Armed with this claim, we can use the universal hash functions from Section 13.6 in our closest­pair algorithm. In expectation, the algorithm will consider O(n) points during the <SPAN class="ft45">Lookup </SPAN>operations. We have to set up multiple hash tables­­a new one each time the minimum distance changes­­and we have to compute O(n) hash­function values. All hash tables are set up for the same size, a prime p ≥ n. We can select one prime and use the same table throughout the algorithm. Using this, we get the following bound on the running time.</P>
<P class="p81 ft45"><SPAN class="ft82">(13.33) </SPAN>In expectation, the algorithm uses O(n) hash­function computations and O(n) additional time for finding the closest pair of points.</P>
<P class="p82 ft76">Note the distinction between this statement and (13.31). There we counted each dictionary operation as a single, atomic step; here,, on the other hand, we’ve conceptually opened up the dictionary operations so as to account for the time incurred due to hash­table collisions and hash­function computations.</P>
<P class="p83 ft2">Finally, consider the time needed for the O(n) hash­function computations. How fast is it to compute the value of a universal hash function <SPAN class="ft48">h</SPAN>? The class of universal hash functions developed in Section 13.6 breaks numbers in our universe U into r ≈ log N/log n smaller numbers of size O(log n) each, and</P>
<P class="p84 ft3">then uses O(r) arithmetic operations on these smaller numbers to compute the hash­function value. $o computing the hash value of a single point involves O(log N/log n) multiplications, on numbers of size log n. This is a total of O(n log N/log n) arithmetic operations over the course of the algorithm, more</P>
<P class="p0 ft21">than the O(n) we were hoping for.</P>
<P class="p85 ft2">In fact, it is possible to decrease the number of arithmetic operations to O(n) by using a more sophisticated class of hash functions. There are other classes of universal hash functions where computing the hash­function value can be done by only O(1) arithmetic operations (though these operations will have to be done on larger numbers, integers of size roughly log N). This class of improved hash functions also comes with one extra difficulty for this application: the hashing scheme needs a prime that is bigger than the size of the universe (rather than just the size of the set of points). Now the universe in this application grows inversely with the minimum distance σ , and so, in particular, it increases every time we discover a new, smaller minimum distance. At such points, we will have to find a new prime and set up a new hash table. Although we will not go into the details of this here, it is possible to deal with these difficulties and make the algorithm achieve an expected running time of O(n).</P>
</DIV>

