<html><head><title>Closest Pair</title><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}.c3{widows:2;orphans:2;text-indent:36pt;text-align:justify;direction:ltr}.c2{widows:2;orphans:2;height:11pt;text-align:justify;direction:ltr}.c5{widows:2;orphans:2;text-align:justify;direction:ltr}.c17{max-width:468pt;background-color:#ffffff;padding:72pt 72pt 72pt 72pt}.c19{margin-right:auto;border-collapse:collapse;margin-left:150pt}.c11{widows:2;orphans:2;direction:ltr}.c4{font-size:9pt;background-color:#ffff00}.c6{font-size:10pt;font-family:"Courier New"}.c16{height:11pt}.c1{font-style:italic}.c12{vertical-align:sub}.c7{margin-left:36pt}.c10{font-size:14pt}.c8{background-color:#ffff00}.c15{margin-left:72pt}.c0{font-family:"Times New Roman"}.c14{vertical-align:super}.c18{text-align:center}.c9{font-weight:bold}.c13{font-size:18pt}.title{widows:2;padding-top:0pt;line-height:1.15;orphans:2;text-align:left;color:#000000;font-size:21pt;font-family:"Trebuchet MS";padding-bottom:0pt;page-break-after:avoid}.subtitle{widows:2;padding-top:0pt;line-height:1.15;orphans:2;text-align:left;color:#666666;font-style:italic;font-size:13pt;font-family:"Trebuchet MS";padding-bottom:10pt;page-break-after:avoid}li{color:#000000;font-size:11pt;font-family:"Arial"}p{color:#000000;font-size:11pt;margin:0;font-family:"Arial"}h1{widows:2;padding-top:10pt;line-height:1.15;orphans:2;text-align:left;color:#000000;font-size:16pt;font-family:"Trebuchet MS";padding-bottom:0pt;page-break-after:avoid}h2{widows:2;padding-top:10pt;line-height:1.15;orphans:2;text-align:left;color:#000000;font-size:13pt;font-family:"Trebuchet MS";font-weight:bold;padding-bottom:0pt;page-break-after:avoid}h3{widows:2;padding-top:8pt;line-height:1.15;orphans:2;text-align:left;color:#666666;font-size:12pt;font-family:"Trebuchet MS";font-weight:bold;padding-bottom:0pt;page-break-after:avoid}h4{widows:2;padding-top:8pt;line-height:1.15;orphans:2;text-align:left;color:#666666;font-size:11pt;text-decoration:underline;font-family:"Trebuchet MS";padding-bottom:0pt;page-break-after:avoid}h5{widows:2;padding-top:8pt;line-height:1.15;orphans:2;text-align:left;color:#666666;font-size:11pt;font-family:"Trebuchet MS";padding-bottom:0pt;page-break-after:avoid}h6{widows:2;padding-top:8pt;line-height:1.15;orphans:2;text-align:left;color:#666666;font-style:italic;font-size:11pt;font-family:"Trebuchet MS";padding-bottom:0pt;page-break-after:avoid}</style></head><body class="c17"><p class="c5"><span class="c0 c9 c13">Finding the Closest Pair of Points: A Randomized Approach<br></span></p><p class="c5"><span class="c0">In this article, </span><span class="c0">we will show how to use randomization to develop an algorithm for the closest pair problem, using an underlying dictionary data structure. We will show that this algorithm runs in O(n) expected time, plus O(n) expected dictionary operations.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">There are several related reasons why it is useful to express the running time of our algorithm in this way, accounting for the dictionary operations separately. We know that dictionaries have a very efficient implementation using hashing, so abstracting out the dictionary operations allows us to treat the hashing as a &quot;black box&quot; and have the algorithm inherit an overall running time from whatever performance guarantee is satisfied by this hashing procedure. A concrete payoff of this is the following. It has been shown that with the right choice of hashing procedure, one can make the underlying dictionary operations run in linear expected time as well, yielding an overall expected running time of O(n). We will talk about the ideas that lead to this O(n) bound at the end of this section.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">It is worth remarking at the outset that randomization shows up for two independent reasons in this algorithm: the way in which the algorithm processes the input points will have a random component, regardless of how the dictionary data structure is implemented; and when the dictionary is implemented</span></p><p class="c5"><span class="c0">using hashing, this introduces an additional source of randomness as part of the hash-table operations. Expressing the running time via the number of dictionary operations allows us to cleanly separate the two uses of randomness.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0 c9 c10">The Problem</span></p><p class="c5"><span class="c0">We are given n points in the plane, and we wish to find the pair that is closest together. This is one of the most basic geometric </span><span class="c1 c0">proximity </span><span class="c0">problems, a topic with a wide range of applications.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">We will use the same notation as in our earlier discussion of the closest-pair problem. We will denote the set of points by </span><span class="c1 c0">P = {p_1,p_2,...p_n}</span><span class="c0">, where </span><span class="c1 c0">p_i</span><span class="c12 c0">&nbsp; </span><span class="c0">has coordinates </span><span class="c1 c0">(x_i,y_i)</span><span class="c0">; and for two points </span><span class="c1 c0">p_i, p_j </span><span class="c0">in </span><span class="c1 c0">P</span><span class="c0">, we use </span><span class="c1 c0">D(p_i,p_j)</span><span class="c0">&nbsp;to denote the standard Euclidean distance between them. Our goal is to find the pair of points </span><span class="c1 c0">p_i,p_j </span><span class="c0">that minimizes </span><span class="c1 c0">D(p_i,p_j)</span><span class="c0">.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">To simplify the discussion, we will assume that the points are all in the unit square: </span><span class="c1 c0">0 &lt;= x_i,y_i &lt; 1</span><span class="c0">&nbsp;for all i = 1,.....,n . This is no loss of generality: in linear time, we can rescale all the </span><span class="c1 c0">x-</span><span class="c0">&nbsp;and </span><span class="c1 c0">y-</span><span class="c0">coordinates of the points so that they lie in a unit square, and then we can translate them so that this unit square has its lower left corner: at the origin.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c10 c0 c9">Designing the Algorithm</span></p><p class="c5"><span class="c0">The basic idea of the algorithm is very simple. We&rsquo;ll consider the points in random order, and maintain a current value </span><span class="c1 c0">d</span><span class="c0">&nbsp;for the closest pair as we process the points in this order. When we get to a new point </span><span class="c1 c0">p</span><span class="c0">, we look &quot;in the vicinity&quot; of </span><span class="c1 c0">p</span><span class="c0">&nbsp;to see if any of the previously considered points are at a distance less than </span><span class="c1 c0">d</span><span class="c0">&nbsp;from </span><span class="c1 c0">p</span><span class="c0">. If not, then the closest pair hasn&rsquo;t changed, and we move on to the next point in the random order. If there is a point within a distance less than </span><span class="c1 c0">d</span><span class="c0">&nbsp;from </span><span class="c1 c0">p</span><span class="c0">, then the closest pair has changed, and we will need to update it. </span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">The challenge in turning this into an efficient algorithm is to figure out how to implement the task of looking for points in the vicinity of </span><span class="c1 c0">p</span><span class="c0">. It is here that the dictionary, data structure will come into play.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">We now begin making this more concrete. Let us assume for simplicity that the points in our random order are labeled </span><span class="c1 c0">p_1,...,p_n</span><span class="c0">. The algorithm proceeds in stages; during each stage, the closest pair remains constant. The first stage starts by setting </span><span class="c1 c0">d=D(p_1,p_2)</span><span class="c0">, the distance of the first two points. The goal of a stage is to either verify that &delta; is indeed the distance of the closest pair of points, or to find a pair of points </span><span class="c1 c0">p_i,p_j</span><span class="c0">&nbsp;with </span><span class="c1 c0">D(p_i,p_j) &lt; d</span><span class="c0">. During a stage, we&rsquo;ll gradually add points in the order </span><span class="c1 c0">p_1,p_2,...,p_n</span><span class="c0">. The stage terminates when we reach a point </span><span class="c1 c0">p_i</span><span class="c0">&nbsp;</span><span class="c0">so that for some </span><span class="c1 c0">j&lt;i, </span><span class="c0">&nbsp;we have </span><span class="c1 c0">D(p_i,p_j)&lt;d</span><span class="c0">. We then let </span><span class="c1 c0">d</span><span class="c0">&nbsp;for the next stage be the closest distance found so far: </span><span class="c1 c0">d=min D(p_i,p_j)</span><span class="c0">, where minimum is taken over all </span><span class="c1 c0">j</span><span class="c0">&nbsp;with </span><span class="c1 c0">j&lt;i</span><span class="c0">.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">The number of stages used will depend on the random order. If we get lucky, and</span><span class="c1 c0">&nbsp;p_1, p_2</span><span class="c0">&nbsp;are the closest pair of points, then a single stage will do. It is also possible to have as many as </span><span class="c1 c0">n-2</span><span class="c0">&nbsp;stages, if adding a new point always decreases the minimum distance. We&rsquo;ll show that the expected running time of the algorithm is within a constant factor of the time needed in the first, lucky case, when the original value of </span><span class="c1 c0">d</span><span class="c0">&nbsp;is the smallest distance.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c1 c0 c9">Testing a Proposed Distance: &nbsp;</span><span class="c0">The main subroutine of the algorithm is a method to test whether the current pair of points with distance </span><span class="c1 c0">d</span><span class="c0">&nbsp;remains the closest pair when a new point is added and, if not, to find the new closest pair.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">The idea of the verification is to subdivide the unit square (the area where the points lie) into subsquares, whose sides have length </span><span class="c1 c0">d/2</span><span class="c0">, as shown in the Figure. Formally, there will be </span><span class="c1 c0">N^2</span><span class="c0">&nbsp;subsquares, where </span><span class="c1 c0">N=1/(2d): for 0&lt;=s&lt;=N-1 and 1&lt;=t&lt;=N-1</span><span class="c0">, we define the subsquare </span><span class="c1 c0">S_{st}</span><span class="c0">&nbsp;as</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c1 c0">S_{st}= {(x,y):sd/2 &lt;=x&lt;(s+1)d/2; td/2&lt;=y&lt;(t+1)d/2}.</span><span class="c0"><br></span></p><p class="c5"><span class="c0">We claim that this collection of subsquares has two nice properties for our purposes. First, any two points that lie in the same subsquare have distance less than </span><span class="c1 c0">d</span><span class="c0">. Second, and a partial converse to this, any two points that are less than </span><span class="c1 c0">d</span><span class="c0">&nbsp;away from each other must fall in either the same subsquare or in very close subsquares.</span></p><p class="c2"><span class="c8 c0"></span></p><p class="c11 c16"><span></span></p><a href="#" name="fb9fb93011ad78ca1e36c4c1e56fdf04d009bd67"></a><a href="#" name="0"></a><table cellpadding="0" cellspacing="0" class="c19"><tbody></tbody></table><p class="c11 c16"><span class="c0"></span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 350.67px;"><img alt="20140918_223244.jpg" src="images/image00.jpg" style="width: 624.00px; height: 350.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c11 c16"><span class="c0"></span></p><p class="c11 c18"><span class="c0 c9">Figure 13.2 Dividing the square into size </span><span class="c1 c0 c9">d/2</span><span class="c0 c9">&nbsp;subsquares. The point </span><span class="c1 c0 c9">p </span><span class="c0 c9">lies in the subsquare S_{st}<br>(The delta in the figure may please be read as </span><span class="c1 c0 c9">d)</span></p><p class="c11 c16 c18"><span class="c8 c0 c9"></span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0 c9">(13.26)</span><span class="c0">&nbsp;</span><span class="c1 c0">If two points p and q belong to the same subsquare S_{st}</span><span class="c1 c0">,</span><span class="c1 c0">&nbsp;then D(p,q)&lt;d.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0 c9">Proof</span><span class="c0">. If points</span><span class="c1 c0">&nbsp;p</span><span class="c0">&nbsp;</span><span class="c0">and </span><span class="c1 c0">q</span><span class="c0">&nbsp;are in the same subsquare, then both coordinates of the two points differ by</span></p><p class="c5"><span class="c0">at most </span><span class="c1 c0">d/2</span><span class="c0">, and hence </span><span class="c1 c0">D(p,q)&lt;=sqrt((d/2)^2+(d/2)^2)=d/sqrt(2)</span><span class="c0">, as required.<br><br>Next, we say that subsquares </span><span class="c1 c0">S_{st}</span><span class="c0">&nbsp;and </span><span class="c1 c0">S_{s&rsquo;t&rsquo;}</span><span class="c0">, are close if </span><span class="c1 c0">|s-s&rsquo;|&lt;=2</span><span class="c0">&nbsp;and |t-t&rsquo;|&lt;=2. (Note that a subsquare is close to itself.)</span><span class="c1 c0"><br></span></p><p class="c5"><span class="c0 c9">(13.27) </span><span class="c1 c0">If for two points p,q in P we have D(p,q)&lt;d</span><span class="c0">,</span><span class="c1 c0">&nbsp;then the subsquares containing them are close.</span></p><p class="c2"><span class="c1 c0"></span></p><p class="c5"><span class="c0 c9">Proof. </span><span class="c0">Consider two points </span><span class="c1 c0">p,q </span><span class="c0">in</span><span class="c1 c0">&nbsp;P</span><span class="c0">&nbsp;belonging to subsquares that are not close; assume </span><span class="c1 c0">p</span><span class="c0">&nbsp;in </span><span class="c1 c0">S_{st}</span><span class="c0">&nbsp;and </span><span class="c1 c0">q </span><span class="c0">in </span><span class="c1 c0">S_{s&rsquo;t&rsquo;}</span><span class="c0">, where one of </span><span class="c1 c0">s, s&rsquo; or t, t&rsquo; </span><span class="c0">differs by more than </span><span class="c1 c0">2</span><span class="c0">. It follows that in one of their respective x- or y-coordinates, </span><span class="c1 c0">p</span><span class="c0">&nbsp;and </span><span class="c1 c0">q</span><span class="c0">&nbsp;differ by at least </span><span class="c1 c0">d</span><span class="c0">, and so we cannot have </span><span class="c1 c0">D(p,q)&lt;d</span><span class="c0">. </span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">Note that for any subsquare </span><span class="c1 c0">S_{st}</span><span class="c0">, the set of subsquares close to it form a 5 x 5 grid around it. Thus we conclude that there are at most 25 subsquares close to </span><span class="c1 c0">S_{st}</span><span class="c0">, counting </span><span class="c1 c0">S_{st}</span><span class="c0">&nbsp;itself. (There will be fewer than 25 if </span><span class="c1 c0">S_{st}</span><span class="c0">&nbsp;</span><span class="c0 c12">&nbsp;</span><span class="c0">is at the edge of the unit square containing the input points.)</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">Statements (13.26) and (13.27) suggest the basic outline of our algorithm. Suppose that, at some point in the algorithm, we have proceeded partway through the random order of the points and seen </span></p><p class="c5"><span class="c1 c0">P&rsquo;</span><span class="c0">&nbsp;is a subset of </span><span class="c1 c0">P</span><span class="c0">, and suppose that we know the minimum distance among points in </span><span class="c1 c0">P&rsquo;</span><span class="c0">&nbsp;to be </span><span class="c1 c0">d</span><span class="c0">. For each of the points in </span><span class="c1 c0">P&rsquo;</span><span class="c0">, we keep track of the subsquare containing it.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">Now, when the next point </span><span class="c1 c0">p</span><span class="c0">&nbsp;is considered, we determine which of the subsquares </span><span class="c1 c0">S_{st}</span><span class="c0">&nbsp;it belongs to. If </span><span class="c1 c0">p</span><span class="c0">&nbsp;is going to cause the minimum distance to change, there must be some earlier point </span><span class="c1 c0">p&rsquo; </span><span class="c0">in</span><span class="c1 c0">&nbsp;P&rsquo;</span><span class="c0">&nbsp;at distance less than </span><span class="c1 c0">d</span><span class="c0">&nbsp;from it; and hence, by (13.27), the point </span><span class="c1 c0">p&rsquo;</span><span class="c0">&nbsp;must be in one of the 25 squares around the square </span><span class="c1 c0">S_{st}</span><span class="c0">&nbsp;containing </span><span class="c1 c0">p.</span><span class="c0">&nbsp;So we will simply check each of these 25 squares one by one to see if it contains a point in </span><span class="c1 c0">P&rsquo;</span><span class="c0">; for each point in </span><span class="c1 c0">P&rsquo;</span><span class="c0">&nbsp;that we find this way, we compute its distance to </span><span class="c1 c0">p</span><span class="c0">. By (13.26), each of these subsquares contains at most one point of </span><span class="c1 c0">P&rsquo;</span><span class="c0">, so this is at most a constant number of distance computations.<br></span></p><p class="c5"><span class="c1 c0 c9">A Data Structure for Maintaining the Subsquares:</span><span class="c0">&nbsp;The high-level description of the algorithm relies on being able to name a subsquare </span><span class="c1 c0">S_{st}</span><span class="c0">&nbsp;and quickly determine which points of </span><span class="c1 c0">P</span><span class="c0">, if any, are contained in it. A dictionary is a natural data structure for implementing such operations. The universe </span><span class="c1 c0">U</span><span class="c0">&nbsp;of possible elements is the set of all subsquares, and the set </span><span class="c1 c0">S</span><span class="c0">&nbsp;maintained by the data structure will be the subsquares that contain points from among the set </span><span class="c1 c0">P&rsquo;</span><span class="c0">&nbsp;that we&rsquo;ve seen so far. Specifically, for each point </span><span class="c1 c0">p&rsquo; </span><span class="c0">in </span><span class="c1 c0">P&rsquo;</span><span class="c0">&nbsp;that we have seen so far, we keep the subsquare containing it in the dictionary, tagged with the index of </span><span class="c1 c0">p&rsquo;</span><span class="c0">. We note that </span><span class="c1 c0">N^2=[1/(2d)]^2</span><span class="c0">&nbsp;will, in general, be much larger than </span><span class="c1 c0">n</span><span class="c0">, the number of points. Thus we are in the type of situation considered in the concept of hashing, where the universe of possible elements (the set of all subsquares) is much larger than the number of elements being indexed (the subsquares containing an input point seen thus far).</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">Now, when we consider the next point </span><span class="c1 c0">p</span><span class="c0">&nbsp;in the random order, we determine the subsquare </span><span class="c1 c0">S_{st}</span><span class="c0">&nbsp;containing it and perform a </span><span class="c1 c0">Lookup</span><span class="c0">&nbsp;operation for each of the 25 subsquares close to </span><span class="c1 c0">S_{st}</span><span class="c0">. For any points discovered by these </span><span class="c1 c0">Lookup </span><span class="c0">operations, we compute the distance to </span><span class="c1 c0">p</span><span class="c0">. If none of these distances are less than </span><span class="c1 c0">d</span><span class="c0">, then the closest distance hasn&rsquo;t changed; we insert </span><span class="c1 c0">S_{st}</span><span class="c12 c0">&nbsp;</span><span class="c0">(tagged with </span><span class="c1 c0">p</span><span class="c0">) </span><span class="c0">into the dictionary and proceed to the next point.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">However, if we find a point </span><span class="c1 c0">p&rsquo;</span><span class="c0">&nbsp;such that </span><span class="c1 c0">d&rsquo;=D(p,p&rsquo;)&lt;d</span><span class="c0">, then we need to update our closest pair. This updating is a rather dramatic activity: Since the value of the closest pair has dropped from </span><span class="c1 c0">d</span><span class="c0">&nbsp;to </span><span class="c1 c0">d&rsquo;</span><span class="c0">, our entire collection of subsquares, and the dictionary supporting it, has become useless--it was, after all, designed only to be useful if the minimum distance was </span><span class="c1 c0">d</span><span class="c0">. We therefore invoke</span><span class="c1 c0">&nbsp;MakeDictionary</span><span class="c0">&nbsp;to create a new, empty dictionary that will hold subsquares whose side lengths are</span></p><p class="c5"><span class="c1 c0">d&rsquo;/2</span><span class="c0">. For each point seen thus far, we determine the subsquare containing it (in this new collection of subsquares), and we insert this subsquare into the dictionary. Having done all this, we are again ready to handle the next point in the random order. <br></span></p><p class="c5"><span class="c0 c9">Summary of the Algorithm: </span><span class="c0">We have now actually described the algorithm in full. To recap:</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c6">Order the points in a random sequence </span><span class="c6 c1">p_1,p_2,...,p_n</span></p><p class="c5"><span class="c6">Let </span><span class="c6 c1">d</span><span class="c6">&nbsp;denote the minimum distance found so far</span></p><p class="c5"><span class="c6">Initialize </span><span class="c6 c1">d:D(p_1,p_2) </span></p><p class="c5"><span class="c6">Invoke </span><span class="c6 c1">MakeDictionary</span><span class="c6">&nbsp;for storing subsquares of side length </span><span class="c6 c1">d/2</span></p><p class="c5"><span class="c6">For </span><span class="c6 c1">i=1, 2 ..... n:</span></p><p class="c3"><span class="c6">Determine the subsquare </span><span class="c6 c1">S_{st}</span><span class="c6">&nbsp;containing </span><span class="c6 c1">p_i</span><span class="c6">&nbsp;</span></p><p class="c3"><span class="c6">Look up the 25 subsquares close to</span><span class="c6 c1">&nbsp;p_i</span></p><p class="c3"><span class="c6">Compute the distance from </span><span class="c6 c1">p_i</span><span class="c6">&nbsp;to any points found in these subsquares</span></p><p class="c3"><span class="c6">If there is a point </span><span class="c6 c1">p_j (j&lt;i)</span><span class="c6">&nbsp;such that </span><span class="c6 c1">d&rsquo;=D(p_j,p_i)&lt;d</span><span class="c6">&nbsp;then</span></p><p class="c3 c7"><span class="c6">Delete the current dictionary</span></p><p class="c3 c7"><span class="c6">Invoke </span><span class="c6 c1">MakeDictionary</span><span class="c6">&nbsp;for storing subsquares of side length </span><span class="c6 c1">d&rsquo;/2</span></p><p class="c3 c7"><span class="c6">For each of the points </span><span class="c6 c1">p_1,p_2,...,p_i</span><span class="c6">:</span></p><p class="c3 c15"><span class="c6">Determine the subsquare of side length </span><span class="c6 c1">d&rsquo;/2</span><span class="c6">&nbsp;</span><span class="c6">that contains it</span></p><p class="c3 c15"><span class="c6">Insert this subsquare into the new dictionary</span></p><p class="c3 c7"><span class="c6">End for</span></p><p class="c3"><span class="c6">Else</span></p><p class="c3 c7"><span class="c6">Insert </span><span class="c6 c1">p_i</span><span class="c6">&nbsp;into the current dictionary</span></p><p class="c3"><span class="c6">Endif</span></p><p class="c5"><span class="c6">Endfor</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c10 c0 c9">Analyzing the Algorithm</span></p><p class="c2"><span class="c0 c9"></span></p><p class="c5"><span class="c0">There are already some things we can say about the overall running time of the algorithm. To consider a new point </span><span class="c1 c0">P_i</span><span class="c0">, we need to perform only a constant number of Lookup operations and a constant number of distance computations. Moreover, even if we had to update the closest pair in every iteration, we&rsquo;d only do </span><span class="c1 c0">n MakeDictionary </span><span class="c0">operations. </span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">The missing ingredient is the total expected cost, over the course of the algorithm&rsquo;s execution, due to reinsertions into new dictionaries when the closest pair is updated. We will consider this next. For now, we can at least summarize the current state of our knowledge as follows:</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0 c9">(13.28)</span><span class="c0">&nbsp;</span><span class="c1 c0">The algorithm correctly maintains the closest pair at all times and it performs at most O(n) distance computations; O(n) Lookup operations, and O(n) MakeDictionary operations.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">We now conclude the analysis by bounding the expected number of </span><span class="c1 c0">Insert</span><span class="c0">&nbsp;operations. Trying to find a good bound on the total expected number of </span><span class="c1 c0">Insert</span><span class="c0">&nbsp;operations seems a bit problematic at first: An update to the closest pair in iteration </span><span class="c1 c0">i</span><span class="c0">&nbsp;will result in </span><span class="c1 c0">i</span><span class="c0">&nbsp;insertions, and so each update comes at a high cost once </span><span class="c1 c0">i</span><span class="c0">&nbsp;gets large. Despite this, we will show the surprising fact that the expected number of insertions is only </span><span class="c1 c0">O(n)</span><span class="c0">. The intuition here is that, even as the cost of updates becomes steeper as the iterations proceed, these updates become correspondingly less likely. </span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">Let </span><span class="c1 c0">X</span><span class="c0">&nbsp;be a random variable specifying the number of </span><span class="c1 c0">Insert</span><span class="c0">&nbsp;operations performed; the value of this random variable is determined by the random order chosen at the outset. We are interested in bounding </span><span class="c1 c0">E[X]</span><span class="c0">, and as usual in this type of situation, it is helpful to break </span><span class="c1 c0">X</span><span class="c0">&nbsp;down into a sum of simpler random variables. Thus let </span><span class="c1 c0">X_i</span><span class="c0">&nbsp;be a random variable equal to 1 if the </span><span class="c1 c0">i</span><span class="c0 c14">th </span><span class="c0">point in the random order causes the minimum distance to change, and equal to 0 otherwise.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">Using these random variables </span><span class="c1 c0">X_i</span><span class="c0">, we can write a simple formula for the total number of </span><span class="c1 c0">Insert </span><span class="c0">operations. Each point is inserted once when it is first encountered; and </span><span class="c1 c0">i</span><span class="c0">&nbsp;points need to be reinserted if the minimum distance changes in iteration </span><span class="c1 c0">i</span><span class="c0">. Thus we have the following claim.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0 c9">(13.29) </span><span class="c1 c0">The total number of Insert operations performed by the algorithm is <br>n+ Sum(i=1 to n, i(X_i)</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">Now we bound the probability </span><span class="c1 c0">Pr[X_i=1]</span><span class="c0">&nbsp;that considering the </span><span class="c1 c0">i</span><span class="c0 c14">th </span><span class="c0">point causes the minimum distance to change.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0 c9">(13.30) </span><span class="c1 c0">Pr [X_i=1]&lt;=2/i</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0 c9">Proof.</span><span class="c0">&nbsp;Consider the first </span><span class="c1 c0">i</span><span class="c0">&nbsp;points </span><span class="c1 c0">p_1, p_2, &hellip;, p_i</span><span class="c0">&nbsp;in the random order. Assume that the minimum distance among these points is achieved by </span><span class="c1 c0">p</span><span class="c0">&nbsp;and </span><span class="c1 c0">q</span><span class="c0">. Now the point </span><span class="c1 c0">p_i</span><span class="c0">&nbsp;can only cause the minimum distance to decrease if </span><span class="c1 c0">p_i=p</span><span class="c0">&nbsp;or </span><span class="c1 c0">p_i=q. </span><span class="c0">Since the first </span><span class="c1 c0">i</span><span class="c0">&nbsp;points are in a random order, any of them is equally likely to be last, so the probability that </span><span class="c1 c0">p</span><span class="c0">&nbsp;or </span><span class="c1 c0">q</span><span class="c0">&nbsp;is last is </span><span class="c1 c0">2/i</span><span class="c0">. </span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">Note that </span><span class="c1 c0">2/i</span><span class="c0">&nbsp;is only an upper bound in (13.30) because there could be multiple pairs among the first </span><span class="c1 c0">i</span><span class="c0">&nbsp;points that define the same smallest distance. </span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">By (13.29) and (13.30), we can bound the total number of </span><span class="c1 c0">Insert </span><span class="c0">operations as</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c1 c0">E[X] = n + Sum(i=1 to n, i.E[x_i]) &lt;= n + 2n = 3n.</span></p><p class="c5 c7"><span class="c8 c0">&nbsp; &nbsp; &nbsp; </span></p><p class="c5"><span class="c0">Combining this with (13.28), we obtain the following bound on the running time of the algorithm.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0 c9">(13.31)</span><span class="c1 c0">&nbsp;In expectation, the randomized closest-pair algorithm requires O(n) time plus O(n) dictionary operations.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c10 c0 c9">Achieving Linear Expected Running Time</span></p><p class="c2"><span class="c10 c0 c9"></span></p><p class="c5"><span class="c0">U</span><span class="c0">p to this point, we have treated the dictionary data structure as a black box, and in (13.31) we bounded the running time of the algorithm in terms of computational time plus dictionary operations. We now want to give a bound on the actual expected running time, and so we need to analyze the work involved in performing these dictionary operations. </span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">To implement the dictionary, we&rsquo;ll use a universal hashing scheme. Once the algorithm employs a hashing scheme, it is making use of randomness in two distinct ways: First, we randomly order the points to be added; and second, for each new minimum distance </span><span class="c1 c0">d</span><span class="c0">, we apply randomization to set up a new hash table using a universal hashing scheme. </span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">When inserting a new point </span><span class="c1 c0">p_i,</span><span class="c0">&nbsp;the algorithm uses the hash-table </span><span class="c1 c0">Lookup</span><span class="c0">&nbsp;operation to find all nodes in the 25 subsquares close to </span><span class="c1 c0">p_i</span><span class="c0">. However, if the hash table has collisions, then these 25 Lookup operations can involve inspecting many more than 25 nodes. The concept of hashing gives us the possibility that each such </span><span class="c1 c0">Lookup </span><span class="c0">operation involves considering </span><span class="c1 c0">O(1)</span><span class="c0">&nbsp;previously inserted points, in expectation. It seems intuitively clear that performing </span><span class="c1 c0">O(n)</span><span class="c0">&nbsp;hash-table operations in expectation, each of which involves considering </span><span class="c1 c0">O(1)</span><span class="c0">&nbsp;elements in expectation, will result in an expected running time of </span><span class="c1 c0">O(n)</span><span class="c0">&nbsp;overall. To make this intuition precise, we need to be careful with how these two sources of randomness interact.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0 c9">(13.32) </span><span class="c1 c0">Assume we implement the randomized closest-pair algorithm using a universal hashing scheme. In expectation, the total number of points considered during the Lookup operations is bounded by O(n).</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0 c9">Proof.</span><span class="c0">&nbsp;From (13.31) we know that the expected number of </span><span class="c1 c0">Lookup</span><span class="c0">&nbsp;operations is </span><span class="c1 c0">O(n)</span><span class="c0">, and from the concept of hashing we know that each of these </span><span class="c1 c0">Lookup </span><span class="c0">operations involves considering only </span><span class="c1 c0">O(1)</span><span class="c0">&nbsp;points in expectation. In order to conclude that this implies the expected number of points considered is </span><span class="c1 c0">O(n)</span><span class="c0">, we now consider the relationship between these two sources of randomness. </span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">Let </span><span class="c1 c0">X</span><span class="c0">&nbsp;be a random variable denoting the number of </span><span class="c1 c0">Lookup</span><span class="c0">&nbsp;operations performed by the algorithm. Now the random order </span><span class="c1 c0">z</span><span class="c0">&nbsp;that the algorithm chooses for the points completely determines the sequence of minimum distance values the algorithm will consider and the sequence of dictionary operations it will perform. As a result, the choice of &sigma; determines the value of </span><span class="c1 c0">X</span><span class="c0">; we let </span><span class="c1 c0">X(z)</span><span class="c0">&nbsp;denote this value, and we let </span><span class="c1 c0">s_z</span><span class="c0">&nbsp;denote the event the algorithm chooses the random order </span><span class="c1 c0">z</span><span class="c0">. Note that the conditional expectation </span><span class="c1 c0">E[X | s_z]</span><span class="c0">&nbsp;is equal to </span><span class="c1 c0">X(z)</span><span class="c0">. Also, by (13.31), we know that </span><span class="c1 c0">E[X]&lt;=(c_0)(n)</span><span class="c0">, </span><span class="c0">for some constant </span><span class="c1 c0">c_0</span><span class="c0">.<br></span></p><p class="c5"><span class="c0">Now consider this sequence of </span><span class="c1 c0">Lookup</span><span class="c0">&nbsp;operations for a fixed order </span><span class="c1 c0">z</span><span class="c0">&nbsp;. For </span><span class="c1 c0">i = 1 ..... X(z)</span><span class="c0">, let </span><span class="c1 c0">Y_i</span><span class="c0">&nbsp;be the number of points that need to be inspected during the i</span><span class="c0 c14">th </span><span class="c1 c0">Lookup</span><span class="c0">&nbsp;operations--namely, the number of previously inserted points that collide with the dictionary entry involved in this </span><span class="c1 c0">Lookup </span><span class="c0">operation. We would like to bound the expected value of </span><span class="c1 c0">Sum(i=1...X(z), Y_i)</span><span class="c0">, where expectation is over both the random choice of </span><span class="c1 c0">z</span><span class="c0">&nbsp;and the random choice of hash function.<br></span></p><p class="c5"><span class="c0">By 13.23, We know that </span><span class="c1 c0">E[Y_i|s_z]=O(1) </span><span class="c0">for all </span><span class="c1 c0">z</span><span class="c0">&nbsp;and all values of </span><span class="c1 c0">i</span><span class="c0">. It is useful to be able to refer to the constant in the expression </span><span class="c1 c0">O(1)</span><span class="c0">&nbsp;here, so we will say that </span><span class="c1 c0">E[Y_i|s_z]&lt;=(c_1)</span><span class="c0">&nbsp;for all </span><span class="c1 c0">z</span><span class="c0">&nbsp;and all values of </span><span class="c1 c0">i</span><span class="c0">. Summing over all </span><span class="c1 c0">i</span><span class="c0">, and using linearity of expectation, we get :<br><br></span><span class="c1 c0">E[sum(i=1...n, Y_i|s_z)]&lt;=(c_1)X(z).</span></p><p class="c2"><span class="c1 c0"></span></p><p class="c2"><span class="c1 c0"></span></p><hr style="page-break-before:always;display:none;"><p class="c2"><span class="c1 c0"></span></p><p class="c5"><span class="c1 c0">Now we have,</span></p><p class="c2"><span class="c1 c0"></span></p><p class="c5"><span class="c1 c0">E[Sum(i=1...X(z), Y_i] </span></p><p class="c3"><span class="c1 c0">= Sum(all z, Pr[s_z].E[sum(i=1...n,Y_i | s_z] </span></p><p class="c5 c7"><span class="c1 c0">&lt;= Sum(all z, Pr[s_z].(c_1).X(z) = c_1. Sum(all z, E[X | s_z].Pr[s_z]=c_1.E[X]</span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0 c8"></span></p><p class="c5"><span class="c0">Since we know that </span><span class="c1 c0">E[X]</span><span class="c0">&nbsp;is at most (</span><span class="c1 c0">c_0).n</span><span class="c0">, the total expected number of points considered is at most (</span><span class="c1 c0">c_0)(c_1)n</span><span class="c0">= </span><span class="c1 c0">O(n)</span><span class="c0">, which proves the claim. </span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">Armed with this claim, we can use the universal hash functions in our closest-pair algorithm. In expectation, the algorithm will consider </span><span class="c1 c0">O(n)</span><span class="c0">&nbsp;points during the </span><span class="c1 c0">Lookup</span><span class="c0">&nbsp;operations. We have to set up multiple hash tables--a new one each time the minimum distance changes--and we have to compute </span><span class="c1 c0">O(n)</span><span class="c0">&nbsp;hash-function values. All hash tables are set up for the same size, a prime </span><span class="c1 c0">p&gt;=n</span><span class="c0">. We can select one prime and use the same table throughout the algorithm. Using this, we get the following bound on the running time.</span></p><p class="c5"><span class="c0">&nbsp;</span></p><p class="c5"><span class="c0 c9">(13.33) </span><span class="c1 c0">In expectation, the algorithm uses O(n) hash-function computations and O(n) additional time for finding the closest pair of points.</span></p><p class="c2"><span class="c1 c0"></span></p><p class="c5"><span class="c0">Note the distinction between this statement and (13.31). There we counted each dictionary operation as a single, atomic step; here,, on the other hand, we&rsquo;ve conceptually opened up the dictionary operations so as to account for the time incurred due to hash-table collisions and hash-function computations.</span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">Finally, consider the time needed for the </span><span class="c1 c0">O(n)</span><span class="c0">&nbsp;hash-function computations. How fast is it to compute the value of a universal hash function </span><span class="c1 c0">h</span><span class="c0">? The class of universal hash functions breaks numbers in our universe </span><span class="c1 c0">U</span><span class="c0">&nbsp;into </span><span class="c1 c0">r</span><span class="c0">&nbsp;approximately equal to </span><span class="c1 c0">log N/log n</span><span class="c0">&nbsp;smaller numbers of size <br></span><span class="c0 c1">O(log n)</span><span class="c0">&nbsp;each, and then uses </span><span class="c1 c0">O(r)</span><span class="c0">&nbsp;arithmetic operations on these smaller numbers to compute the hash-function value. So computing the hash value of a single point involves </span><span class="c1 c0">O(log N/log n)</span><span class="c0">&nbsp;multiplications, on numbers of size </span><span class="c1 c0">log n</span><span class="c0">. This is a total of </span><span class="c1 c0">O(n log N/log n)</span><span class="c0">&nbsp;arithmetic operations over the course of the algorithm, more</span></p><p class="c5"><span class="c0">than the </span><span class="c1 c0">O(n)</span><span class="c0">&nbsp;we were hoping for. </span></p><p class="c2"><span class="c0"></span></p><p class="c5"><span class="c0">In fact, it is possible to decrease the number of arithmetic operations to </span><span class="c1 c0">O(n)</span><span class="c0">&nbsp;by using a more sophisticated class of hash functions. There are other classes of universal hash functions where computing the hash-function value can be done by only </span><span class="c1 c0">O(1)</span><span class="c0">&nbsp;arithmetic operations (though these operations will have to be done on larger numbers, integers of size roughly </span><span class="c1 c0">log N</span><span class="c0">). This class of improved hash functions also comes with one extra difficulty for this application: the hashing scheme needs a prime that is bigger than the size of the universe (rather than just the size of the set of points). Now the universe in this application grows inversely with the minimum distance </span><span class="c1 c0">z</span><span class="c0">, and so, in particular, it increases every time we discover a new, smaller minimum distance. At such points, we will have to find a new prime and set up a new hash table. Although we will not go into the details of this here, it is possible to deal with these difficulties and make the algorithm achieve an expected running time of </span><span class="c1 c0">O(n)</span><span class="c0">.</span></p><p class="c2"><span class="c0"></span></p><p class="c2"><span class="c0"></span></p></body></html>